{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from isolation import Board\n",
    "from sample_players import RandomPlayer\n",
    "from sample_players import null_score\n",
    "from sample_players import open_move_score\n",
    "from sample_players import improved_score\n",
    "from game_agent import CustomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_score(game, player, agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "    player_moves = game.get_legal_moves(player)\n",
    "    opp_moves = game.get_legal_moves(game.get_opponent(player))\n",
    "    return float(agent_weight*len(player_moves)**agent_exp + agent_base**len(player_moves) - \n",
    "                 opp_weight*len(opp_moves)**opp_exp + opp_base**len(opp_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "from math import log\n",
    "\n",
    "class Timeout(Exception):\n",
    "    \"\"\"Subclass base exception for code clarity.\"\"\"\n",
    "    pass\n",
    "\n",
    "class CustomPlayer1:\n",
    "    \"\"\"Game-playing agent that chooses a move using your evaluation function\n",
    "    and a depth-limited minimax algorithm with alpha-beta pruning. You must\n",
    "    finish and test this player to make sure it properly uses minimax and\n",
    "    alpha-beta to return a good move before the search time limit expires.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_depth : int (optional)\n",
    "        A strictly positive integer (i.e., 1, 2, 3,...) for the number of\n",
    "        layers in the game tree to explore for fixed-depth search. (i.e., a\n",
    "        depth of one (1) would only explore the immediate sucessors of the\n",
    "        current state.)\n",
    "\n",
    "    score_fn : callable (optional)\n",
    "        A function to use for heuristic evaluation of game states.\n",
    "\n",
    "    iterative : boolean (optional)\n",
    "        Flag indicating whether to perform fixed-depth search (False) or\n",
    "        iterative deepening search (True).\n",
    "\n",
    "    method : {'minimax', 'alphabeta'} (optional)\n",
    "        The name of the search method to use in get_move().\n",
    "\n",
    "    timeout : float (optional)\n",
    "        Time remaining (in milliseconds) when search is aborted. Should be a\n",
    "        positive value large enough to allow the function to return before the\n",
    "        timer expires.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, search_depth=3, score_fn=custom_score,\n",
    "                 iterative=True, method='minimax', timeout=10., \n",
    "                 agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "        self.search_depth = search_depth\n",
    "        self.iterative = iterative\n",
    "        self.score = score_fn\n",
    "        self.method = method\n",
    "        self.time_left = None\n",
    "        self.TIMER_THRESHOLD = timeout\n",
    "        self.agent_weight = agent_weight\n",
    "        self.agent_exp = agent_exp\n",
    "        self.agent_base = agent_base\n",
    "        self.opp_weight = opp_weight\n",
    "        self.opp_exp = opp_exp\n",
    "        self.opp_base = opp_base\n",
    "\n",
    "    def get_move(self, game, legal_moves, time_left):\n",
    "        \"\"\"Search for the best move from the available legal moves and return a\n",
    "        result before the time limit expires.\n",
    "\n",
    "        This function must perform iterative deepening if self.iterative=True,\n",
    "        and it must use the search method (minimax or alphabeta) corresponding\n",
    "        to the self.method value.\n",
    "\n",
    "        **********************************************************************\n",
    "        NOTE: If time_left < 0 when this function returns, the agent will\n",
    "              forfeit the game due to timeout. You must return _before_ the\n",
    "              timer reaches 0.\n",
    "        **********************************************************************\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : `isolation.Board`\n",
    "            An instance of `isolation.Board` encoding the current state of the\n",
    "            game (e.g., player locations and blocked cells).\n",
    "\n",
    "        legal_moves : list<(int, int)>\n",
    "            A list containing legal moves. Moves are encoded as tuples of pairs\n",
    "            of ints defining the next (row, col) for the agent to occupy.\n",
    "\n",
    "        time_left : callable\n",
    "            A function that returns the number of milliseconds left in the\n",
    "            current turn. Returning with any less than 0 ms remaining forfeits\n",
    "            the game.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int, int)\n",
    "            Board coordinates corresponding to a legal move; may return\n",
    "            (-1, -1) if there are no available legal moves.\n",
    "        \"\"\"\n",
    "\n",
    "        self.time_left = time_left\n",
    "\n",
    "        # TODO: finish this function!\n",
    "\n",
    "        # Perform any required initializations, including selecting an initial\n",
    "        # move from the game board (i.e., an opening book), or returning\n",
    "        # immediately if there are no legal moves\n",
    "        depth = 1\n",
    "        best_move = (-1,-1)\n",
    "        try:\n",
    "            # The search method call (alpha beta or minimax) should happen in\n",
    "            # here in order to avoid timeout. The try/except block will\n",
    "            # automatically catch the exception raised by the search method\n",
    "            # when the timer gets close to expiring\n",
    "            if self.iterative:\n",
    "                if self.method=='minimax':\n",
    "                    while True:\n",
    "                        best_score, best_move = self.minimax(game, depth)\n",
    "                        depth+=1\n",
    "                else:\n",
    "                    while True:\n",
    "                        best_score, best_move = self.alphabeta(game, depth)\n",
    "                        depth+=1\n",
    "            else:\n",
    "                if self.method=='minimax':\n",
    "                    best_score, best_move = self.minimax(game, self.search_depth)\n",
    "                else:\n",
    "                    best_score, best_move = self.alphabeta(game, self.search_depth)\n",
    "\n",
    "        except Timeout:\n",
    "            # Handle any actions required at timeout, if necessary\n",
    "            return best_move\n",
    "\n",
    "        # Return the best move from the last completed search iteration\n",
    "        return best_move\n",
    "    \n",
    "    def minimax(self, game, depth, maximizing_player=True):\n",
    "        \"\"\"Implement the minimax search algorithm as described in the lectures.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : isolation.Board\n",
    "            An instance of the Isolation game `Board` class representing the\n",
    "            current game state\n",
    "\n",
    "        depth : int\n",
    "            Depth is an integer representing the maximum number of plies to\n",
    "            search in the game tree before aborting\n",
    "\n",
    "        maximizing_player : bool\n",
    "            Flag indicating whether the current search depth corresponds to a\n",
    "            maximizing layer (True) or a minimizing layer (False)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The score for the current search branch\n",
    "\n",
    "        tuple(int, int)\n",
    "            The best move for the current branch; (-1, -1) for no legal moves\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "            (1) You MUST use the `self.score()` method for board evaluation\n",
    "                to pass the project unit tests; you cannot call any other\n",
    "                evaluation function directly.\n",
    "        \"\"\"\n",
    "        # TERMINAL-TEST\n",
    "        if game.is_winner(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), game.get_player_location(self)\n",
    "        if game.is_loser(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), (-1,-1)\n",
    "        def max_value(game, current_depth):\n",
    "            \"\"\"Finds the best move for agent at current_depth.\n",
    "            \"\"\"\n",
    "            if self.time_left() < self.TIMER_THRESHOLD:\n",
    "                raise Timeout()\n",
    "            if game.is_winner(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if game.is_loser(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if current_depth >= depth:  \n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            score = float(\"-inf\")\n",
    "            for m in game.get_legal_moves(self):\n",
    "                score = max(score, min_value(game.forecast_move(m), current_depth+1))\n",
    "            return score\n",
    "        def min_value(game, current_depth):\n",
    "            \"\"\"Finds the best move for opponent at current_depth.\n",
    "            \"\"\"\n",
    "            if self.time_left() < self.TIMER_THRESHOLD:\n",
    "                raise Timeout()\n",
    "            if game.is_winner(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if game.is_loser(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if current_depth >= depth:\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            score = float(\"inf\")\n",
    "            for m in game.get_legal_moves():\n",
    "                score = min(score, max_value(game.forecast_move(m), current_depth+1))\n",
    "            return score\n",
    "        moves = game.get_legal_moves()\n",
    "        imm_scores = [min_value(game.forecast_move(m),1) for m in moves]\n",
    "        best_score = max(imm_scores)\n",
    "        best_move = moves[imm_scores.index(best_score)]\n",
    "        return best_score, best_move\n",
    "\n",
    "    def alphabeta(self, game, depth, alpha=float(\"-inf\"), beta=float(\"inf\"), maximizing_player=True):\n",
    "        \"\"\"Implement minimax search with alpha-beta pruning as described in the\n",
    "        lectures.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : isolation.Board\n",
    "            An instance of the Isolation game `Board` class representing the\n",
    "            current game state\n",
    "\n",
    "        depth : int\n",
    "            Depth is an integer representing the maximum number of plies to\n",
    "            search in the game tree before aborting\n",
    "\n",
    "        alpha : float\n",
    "            Alpha limits the lower bound of search on minimizing layers\n",
    "\n",
    "        beta : float\n",
    "            Beta limits the upper bound of search on maximizing layers\n",
    "\n",
    "        maximizing_player : bool\n",
    "            Flag indicating whether the current search depth corresponds to a\n",
    "            maximizing layer (True) or a minimizing layer (False)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The score for the current search branch\n",
    "\n",
    "        tuple(int, int)\n",
    "            The best move for the current branch; (-1, -1) for no legal moves\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "            (1) You MUST use the `self.score()` method for board evaluation\n",
    "                to pass the project unit tests; you cannot call any other\n",
    "                evaluation function directly.\n",
    "        \"\"\"\n",
    "        if self.time_left() < self.TIMER_THRESHOLD:\n",
    "            raise Timeout()\n",
    "        if game.is_winner(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), game.get_player_location(self)\n",
    "        if game.is_loser(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), (-1,-1)\n",
    "        low_score, high_score = float(\"inf\"), float('-inf')\n",
    "        best_move = (-1,-1)\n",
    "        if depth == 1:\n",
    "            if maximizing_player:\n",
    "                for move in game.get_legal_moves(self):\n",
    "                    score = self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "                    if score >= beta:\n",
    "                        return score, move\n",
    "                    if score > high_score:\n",
    "                        high_score, best_move = score, move\n",
    "                return high_score, best_move\n",
    "            else:\n",
    "                for move in game.get_legal_moves():\n",
    "                    score = self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "                    if score <= alpha:\n",
    "                        return score, move\n",
    "                    if score < low_score:\n",
    "                        low_score, best_move = score, move\n",
    "                return low_score, best_move\n",
    "        if maximizing_player:\n",
    "            for move in game.get_legal_moves(self):\n",
    "                score, _ = self.alphabeta(game.forecast_move(move), depth-1, alpha, beta, maximizing_player=False)\n",
    "                if score>=beta:\n",
    "                    return score, move\n",
    "                if score>high_score:\n",
    "                    high_score, best_move = score, move\n",
    "                alpha = max(high_score, alpha)\n",
    "            return high_score, best_move\n",
    "        else:\n",
    "            for move in game.get_legal_moves():\n",
    "                score, _ = self.alphabeta(game.forecast_move(move), depth-1, alpha, beta, maximizing_player=True)\n",
    "                if score<=alpha:\n",
    "                    return score, move\n",
    "                if score<low_score:\n",
    "                    low_score, best_move = score, move\n",
    "                beta = min(low_score, beta)\n",
    "            return low_score, best_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_MATCHES = 5  # number of matches against each opponent\n",
    "TIME_LIMIT = 150  # number of milliseconds before timeout\n",
    "\n",
    "TIMEOUT_WARNING = \"One or more agents lost a match this round due to \" + \\\n",
    "                  \"timeout. The get_move() function must return before \" + \\\n",
    "                  \"time_left() reaches 0 ms. You will need to leave some \" + \\\n",
    "                  \"time for the function to return, and may need to \" + \\\n",
    "                  \"increase this margin to avoid timeouts during  \" + \\\n",
    "                  \"tournament play.\"\n",
    "\n",
    "DESCRIPTION = \"\"\"\n",
    "This script evaluates the performance of the custom heuristic function by\n",
    "comparing the strength of an agent using iterative deepening (ID) search with\n",
    "alpha-beta pruning against the strength rating of agents using other heuristic\n",
    "functions.  The `ID_Improved` agent provides a baseline by measuring the\n",
    "performance of a basic agent using Iterative Deepening and the \"improved\"\n",
    "heuristic (from lecture) on your hardware.  The `Student` agent then measures\n",
    "the performance of Iterative Deepening and the custom heuristic against the\n",
    "same opponents.\n",
    "\"\"\"\n",
    "\n",
    "Agent = namedtuple(\"Agent\", [\"player\", \"name\"])\n",
    "\n",
    "\n",
    "def play_match(player1, player2):\n",
    "    \"\"\"\n",
    "    Play a \"fair\" set of matches between two agents by playing two games\n",
    "    between the players, forcing each agent to play from randomly selected\n",
    "    positions. This should control for differences in outcome resulting from\n",
    "    advantage due to starting position on the board.\n",
    "    \"\"\"\n",
    "    num_wins = {player1: 0, player2: 0}\n",
    "    num_timeouts = {player1: 0, player2: 0}\n",
    "    num_invalid_moves = {player1: 0, player2: 0}\n",
    "    games = [Board(player1, player2), Board(player2, player1)]\n",
    "\n",
    "    # initialize both games with a random move and response\n",
    "    for _ in range(2):\n",
    "        move = random.choice(games[0].get_legal_moves())\n",
    "        games[0].apply_move(move)\n",
    "        games[1].apply_move(move)\n",
    "\n",
    "    # play both games and tally the results\n",
    "    for game in games:\n",
    "        winner, _, termination = game.play(time_limit=TIME_LIMIT)\n",
    "\n",
    "        if player1 == winner:\n",
    "            num_wins[player1] += 1\n",
    "\n",
    "            if termination == \"timeout\":\n",
    "                num_timeouts[player2] += 1\n",
    "            else:\n",
    "                num_invalid_moves[player2] += 1\n",
    "\n",
    "        elif player2 == winner:\n",
    "\n",
    "            num_wins[player2] += 1\n",
    "\n",
    "            if termination == \"timeout\":\n",
    "                num_timeouts[player1] += 1\n",
    "            else:\n",
    "                num_invalid_moves[player1] += 1\n",
    "\n",
    "    if sum(num_timeouts.values()) != 0:\n",
    "        warnings.warn(TIMEOUT_WARNING)\n",
    "\n",
    "    return num_wins[player1], num_wins[player2]\n",
    "\n",
    "\n",
    "def play_round(agents, num_matches):\n",
    "    \"\"\"\n",
    "    Play one round (i.e., a single match between each pair of opponents)\n",
    "    \"\"\"\n",
    "    agent_1 = agents[-1]\n",
    "    wins = 0.\n",
    "    total = 0.\n",
    "\n",
    "    print(\"\\nPlaying Matches:\")\n",
    "    print(\"----------\")\n",
    "\n",
    "    for idx, agent_2 in enumerate(agents[:-1]):\n",
    "\n",
    "        counts = {agent_1.player: 0., agent_2.player: 0.}\n",
    "        names = [agent_1.name, agent_2.name]\n",
    "        print(\"  Match {}: {!s:^11} vs {!s:^11}\".format(idx + 1, *names), end=' ')\n",
    "\n",
    "        # Each player takes a turn going first\n",
    "        for p1, p2 in itertools.permutations((agent_1.player, agent_2.player)):\n",
    "            for _ in range(num_matches):\n",
    "                score_1, score_2 = play_match(p1, p2)\n",
    "                counts[p1] += score_1\n",
    "                counts[p2] += score_2\n",
    "                total += score_1 + score_2\n",
    "\n",
    "        wins += counts[agent_1.player]\n",
    "\n",
    "        print(\"\\tResult: {} to {}\".format(int(counts[agent_1.player]),\n",
    "                                          int(counts[agent_2.player])))\n",
    "\n",
    "    return 100. * wins / total\n",
    "\n",
    "def main(agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "\n",
    "    HEURISTICS = [(\"Null\", null_score),\n",
    "                  (\"Open\", open_move_score),\n",
    "                  (\"Improved\", improved_score)]\n",
    "    AB_ARGS = {\"search_depth\": 5, \"method\": 'alphabeta', \"iterative\": False}\n",
    "    MM_ARGS = {\"search_depth\": 3, \"method\": 'minimax', \"iterative\": False}\n",
    "    CUSTOM_ARGS = {\"method\": 'alphabeta', 'iterative': True}\n",
    "    CUSTOM_ARGS1 = {\"method\": 'alphabeta', 'iterative': True, \n",
    "                   'agent_weight':agent_weight, 'agent_exp':agent_exp, 'agent_base':agent_base, \n",
    "                   'opp_weight':opp_weight, 'opp_exp':opp_exp, 'opp_base':opp_base}\n",
    "\n",
    "    # Create a collection of CPU agents using fixed-depth minimax or alpha beta\n",
    "    # search, or random selection.  The agent names encode the search method\n",
    "    # (MM=minimax, AB=alpha-beta) and the heuristic function (Null=null_score,\n",
    "    # Open=open_move_score, Improved=improved_score). For example, MM_Open is\n",
    "    # an agent using minimax search with the open moves heuristic.\n",
    "    mm_agents = [Agent(CustomPlayer(score_fn=h, **MM_ARGS),\n",
    "                       \"MM_\" + name) for name, h in HEURISTICS]\n",
    "    ab_agents = [Agent(CustomPlayer(score_fn=h, **AB_ARGS),\n",
    "                       \"AB_\" + name) for name, h in HEURISTICS]\n",
    "    random_agents = [Agent(RandomPlayer(), \"Random\")]\n",
    "\n",
    "    # ID_Improved agent is used for comparison to the performance of the\n",
    "    # submitted agent for calibration on the performance across different\n",
    "    # systems; i.e., the performance of the student agent is considered\n",
    "    # relative to the performance of the ID_Improved agent to account for\n",
    "    # faster or slower computers.\n",
    "    test_agents = [Agent(CustomPlayer(score_fn=improved_score, **CUSTOM_ARGS), \"ID_Improved\"),\n",
    "                   Agent(CustomPlayer1(score_fn=custom_score, **CUSTOM_ARGS), \"Student\")]\n",
    "    print(CUSTOM_ARGS1)\n",
    "\n",
    "    print(DESCRIPTION)\n",
    "    win_ratios = []\n",
    "    for agentUT in test_agents:\n",
    "        print(\"\")\n",
    "        print(\"*************************\")\n",
    "        print(\"{:^25}\".format(\"Evaluating: \" + agentUT.name))\n",
    "        print(\"*************************\")\n",
    "\n",
    "        agents = random_agents + mm_agents + ab_agents + [agentUT]\n",
    "        win_ratio = play_round(agents, NUM_MATCHES)\n",
    "        win_ratios.append(win_ratio)\n",
    "        print(\"\\n\\nResults:\")\n",
    "        print(\"----------\")\n",
    "        print(\"{!s:<15}{:>10.2f}%\".format(agentUT.name, win_ratio))\n",
    "    return win_ratios[1]/win_ratios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   agent_base |   agent_exp |   agent_weight |   opp_base |   opp_exp |   opp_weight | \n",
      "{'method': 'alphabeta', 'iterative': True, 'agent_weight': 1, 'agent_exp': 2, 'agent_base': 0, 'opp_weight': 2, 'opp_exp': 1, 'opp_base': 0}\n",
      "\n",
      "This script evaluates the performance of the custom heuristic function by\n",
      "comparing the strength of an agent using iterative deepening (ID) search with\n",
      "alpha-beta pruning against the strength rating of agents using other heuristic\n",
      "functions.  The `ID_Improved` agent provides a baseline by measuring the\n",
      "performance of a basic agent using Iterative Deepening and the \"improved\"\n",
      "heuristic (from lecture) on your hardware.  The `Student` agent then measures\n",
      "the performance of Iterative Deepening and the custom heuristic against the\n",
      "same opponents.\n",
      "\n",
      "\n",
      "*************************\n",
      " Evaluating: ID_Improved \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1: ID_Improved vs   Random    \tResult: 17 to 3\n",
      "  Match 2: ID_Improved vs   MM_Null   \tResult: 13 to 7\n",
      "  Match 3: ID_Improved vs   MM_Open   \tResult: 16 to 4\n",
      "  Match 4: ID_Improved vs MM_Improved \tResult: 12 to 8\n",
      "  Match 5: ID_Improved vs   AB_Null   \tResult: 15 to 5\n",
      "  Match 6: ID_Improved vs   AB_Open   \tResult: 13 to 7\n",
      "  Match 7: ID_Improved vs AB_Improved \tResult: 14 to 6\n",
      "\n",
      "\n",
      "Results:\n",
      "----------\n",
      "ID_Improved         71.43%\n",
      "\n",
      "*************************\n",
      "   Evaluating: Student   \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1:   Student   vs   Random    \tResult: 18 to 2\n",
      "  Match 2:   Student   vs   MM_Null   \tResult: 17 to 3\n",
      "  Match 3:   Student   vs   MM_Open   \tResult: 14 to 6\n",
      "  Match 4:   Student   vs MM_Improved \tResult: 12 to 8\n",
      "  Match 5:   Student   vs   AB_Null   \tResult: 14 to 6\n",
      "  Match 6:   Student   vs   AB_Open   \tResult: 12 to 8\n",
      "  Match 7:   Student   vs AB_Improved \tResult: 14 to 6\n",
      "\n",
      "\n",
      "Results:\n",
      "----------\n",
      "Student             72.14%\n",
      "    1 | 10m47s | \u001b[35m   1.01000\u001b[0m | \u001b[32m      0.0000\u001b[0m | \u001b[32m     2.0000\u001b[0m | \u001b[32m        1.0000\u001b[0m | \u001b[32m    0.0000\u001b[0m | \u001b[32m   1.0000\u001b[0m | \u001b[32m      2.0000\u001b[0m | \n",
      "{'method': 'alphabeta', 'iterative': True, 'agent_weight': 2, 'agent_exp': 1, 'agent_base': 0, 'opp_weight': 1, 'opp_exp': 2, 'opp_base': 0}\n",
      "\n",
      "This script evaluates the performance of the custom heuristic function by\n",
      "comparing the strength of an agent using iterative deepening (ID) search with\n",
      "alpha-beta pruning against the strength rating of agents using other heuristic\n",
      "functions.  The `ID_Improved` agent provides a baseline by measuring the\n",
      "performance of a basic agent using Iterative Deepening and the \"improved\"\n",
      "heuristic (from lecture) on your hardware.  The `Student` agent then measures\n",
      "the performance of Iterative Deepening and the custom heuristic against the\n",
      "same opponents.\n",
      "\n",
      "\n",
      "*************************\n",
      " Evaluating: ID_Improved \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1: ID_Improved vs   Random    \tResult: 18 to 2\n",
      "  Match 2: ID_Improved vs   MM_Null   \tResult: 16 to 4\n",
      "  Match 3: ID_Improved vs   MM_Open   \tResult: 14 to 6\n",
      "  Match 4: ID_Improved vs MM_Improved \tResult: 13 to 7\n",
      "  Match 5: ID_Improved vs   AB_Null   \tResult: 9 to 11\n",
      "  Match 6: ID_Improved vs   AB_Open   \tResult: 11 to 9\n",
      "  Match 7: ID_Improved vs AB_Improved \tResult: 11 to 9\n",
      "\n",
      "\n",
      "Results:\n",
      "----------\n",
      "ID_Improved         65.71%\n",
      "\n",
      "*************************\n",
      "   Evaluating: Student   \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1:   Student   vs   Random    \tResult: 18 to 2\n",
      "  Match 2:   Student   vs   MM_Null   \tResult: 16 to 4\n",
      "  Match 3:   Student   vs   MM_Open   \tResult: 14 to 6\n",
      "  Match 4:   Student   vs MM_Improved \tResult: 10 to 10\n",
      "  Match 5:   Student   vs   AB_Null   \tResult: 13 to 7\n",
      "  Match 6:   Student   vs   AB_Open   \tResult: 12 to 8\n",
      "  Match 7:   Student   vs AB_Improved "
     ]
    }
   ],
   "source": [
    "params={'agent_weight':(0,100), 'agent_exp':(0,10), 'agent_base':(0,10),\n",
    "        'opp_weight':(0,100), 'opp_exp':(0,10), 'opp_base':(0,10)}\n",
    "BO = BayesianOptimization(main, params)\n",
    "BO.explore({'agent_weight':[1,2,0,0], 'agent_exp':[2,1,1,1], 'agent_base':[0,0,3,2],\n",
    "           'opp_weight':[2,1,0,0], 'opp_exp':[1,2,1,1], 'opp_base':[0,0,2,.5]})\n",
    "BO.maximize(init_points=3, n_iter=100, acq='ucb', kappa=2.576*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_params': {'agent_base': 7.4921842058035448,\n",
       "  'agent_exp': 7.4205991775955464,\n",
       "  'agent_weight': 82.874576048706842,\n",
       "  'opp_base': 5.7974792415172294,\n",
       "  'opp_exp': 7.6446303872568819,\n",
       "  'opp_weight': 73.420012687089994},\n",
       " 'max_val': 1.1363636363636365}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO.res['max'] # First run, acq='ei'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py:258: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 60 but corresponding boolean dimension is 61\n",
      "  self.gp.fit(self.X[ur], self.Y[ur])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 60 is out of bounds for axis 1 with size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-816bf4b0af09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.576\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Find unique rows of X to avoid GP from breaking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 60 is out of bounds for axis 1 with size 60"
     ]
    }
   ],
   "source": [
    "BO.res['max'] # Second run, including acq='ucb',\n",
    "# BO.explore({'agent_weight':[1,2,0,0], 'agent_exp':[2,1,1,1], 'agent_base':[0,0,3,2],\n",
    "#           'opp_weight':[2,1,0,0], 'opp_exp':[1,2,1,1], 'opp_base':[0,0,2,.5]}),\n",
    "# init_points=3, n_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
