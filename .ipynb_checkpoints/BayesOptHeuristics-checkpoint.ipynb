{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from isolation import Board\n",
    "from sample_players import RandomPlayer\n",
    "from sample_players import null_score\n",
    "from sample_players import open_move_score\n",
    "from sample_players import improved_score\n",
    "from game_agent import CustomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def custom_score(game, player, agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "    player_moves = game.get_legal_moves(player)\n",
    "    opp_moves = game.get_legal_moves(game.get_opponent(player))\n",
    "    return float(agent_weight*len(player_moves)**agent_exp + agent_base**len(player_moves) - \n",
    "                 opp_weight*len(opp_moves)**opp_exp + opp_base**len(opp_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "from math import log\n",
    "\n",
    "class Timeout(Exception):\n",
    "    \"\"\"Subclass base exception for code clarity.\"\"\"\n",
    "    pass\n",
    "\n",
    "class CustomPlayer1:\n",
    "    \"\"\"Game-playing agent that chooses a move using your evaluation function\n",
    "    and a depth-limited minimax algorithm with alpha-beta pruning. You must\n",
    "    finish and test this player to make sure it properly uses minimax and\n",
    "    alpha-beta to return a good move before the search time limit expires.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_depth : int (optional)\n",
    "        A strictly positive integer (i.e., 1, 2, 3,...) for the number of\n",
    "        layers in the game tree to explore for fixed-depth search. (i.e., a\n",
    "        depth of one (1) would only explore the immediate sucessors of the\n",
    "        current state.)\n",
    "\n",
    "    score_fn : callable (optional)\n",
    "        A function to use for heuristic evaluation of game states.\n",
    "\n",
    "    iterative : boolean (optional)\n",
    "        Flag indicating whether to perform fixed-depth search (False) or\n",
    "        iterative deepening search (True).\n",
    "\n",
    "    method : {'minimax', 'alphabeta'} (optional)\n",
    "        The name of the search method to use in get_move().\n",
    "\n",
    "    timeout : float (optional)\n",
    "        Time remaining (in milliseconds) when search is aborted. Should be a\n",
    "        positive value large enough to allow the function to return before the\n",
    "        timer expires.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, search_depth=3, score_fn=custom_score,\n",
    "                 iterative=True, method='minimax', timeout=10., \n",
    "                 agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "        self.search_depth = search_depth\n",
    "        self.iterative = iterative\n",
    "        self.score = score_fn\n",
    "        self.method = method\n",
    "        self.time_left = None\n",
    "        self.TIMER_THRESHOLD = timeout\n",
    "        self.agent_weight = agent_weight\n",
    "        self.agent_exp = agent_exp\n",
    "        self.agent_base = agent_base\n",
    "        self.opp_weight = opp_weight\n",
    "        self.opp_exp = opp_exp\n",
    "        self.opp_base = opp_base\n",
    "\n",
    "    def get_move(self, game, legal_moves, time_left):\n",
    "        \"\"\"Search for the best move from the available legal moves and return a\n",
    "        result before the time limit expires.\n",
    "\n",
    "        This function must perform iterative deepening if self.iterative=True,\n",
    "        and it must use the search method (minimax or alphabeta) corresponding\n",
    "        to the self.method value.\n",
    "\n",
    "        **********************************************************************\n",
    "        NOTE: If time_left < 0 when this function returns, the agent will\n",
    "              forfeit the game due to timeout. You must return _before_ the\n",
    "              timer reaches 0.\n",
    "        **********************************************************************\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : `isolation.Board`\n",
    "            An instance of `isolation.Board` encoding the current state of the\n",
    "            game (e.g., player locations and blocked cells).\n",
    "\n",
    "        legal_moves : list<(int, int)>\n",
    "            A list containing legal moves. Moves are encoded as tuples of pairs\n",
    "            of ints defining the next (row, col) for the agent to occupy.\n",
    "\n",
    "        time_left : callable\n",
    "            A function that returns the number of milliseconds left in the\n",
    "            current turn. Returning with any less than 0 ms remaining forfeits\n",
    "            the game.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (int, int)\n",
    "            Board coordinates corresponding to a legal move; may return\n",
    "            (-1, -1) if there are no available legal moves.\n",
    "        \"\"\"\n",
    "\n",
    "        self.time_left = time_left\n",
    "\n",
    "        # TODO: finish this function!\n",
    "\n",
    "        # Perform any required initializations, including selecting an initial\n",
    "        # move from the game board (i.e., an opening book), or returning\n",
    "        # immediately if there are no legal moves\n",
    "        depth = 1\n",
    "        best_move = (-1,-1)\n",
    "        try:\n",
    "            # The search method call (alpha beta or minimax) should happen in\n",
    "            # here in order to avoid timeout. The try/except block will\n",
    "            # automatically catch the exception raised by the search method\n",
    "            # when the timer gets close to expiring\n",
    "            if self.iterative:\n",
    "                if self.method=='minimax':\n",
    "                    while True:\n",
    "                        best_score, best_move = self.minimax(game, depth)\n",
    "                        depth+=1\n",
    "                else:\n",
    "                    while True:\n",
    "                        best_score, best_move = self.alphabeta(game, depth)\n",
    "                        depth+=1\n",
    "            else:\n",
    "                if self.method=='minimax':\n",
    "                    best_score, best_move = self.minimax(game, self.search_depth)\n",
    "                else:\n",
    "                    best_score, best_move = self.alphabeta(game, self.search_depth)\n",
    "\n",
    "        except Timeout:\n",
    "            # Handle any actions required at timeout, if necessary\n",
    "            return best_move\n",
    "\n",
    "        # Return the best move from the last completed search iteration\n",
    "        return best_move\n",
    "    \n",
    "    def minimax(self, game, depth, maximizing_player=True):\n",
    "        \"\"\"Implement the minimax search algorithm as described in the lectures.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : isolation.Board\n",
    "            An instance of the Isolation game `Board` class representing the\n",
    "            current game state\n",
    "\n",
    "        depth : int\n",
    "            Depth is an integer representing the maximum number of plies to\n",
    "            search in the game tree before aborting\n",
    "\n",
    "        maximizing_player : bool\n",
    "            Flag indicating whether the current search depth corresponds to a\n",
    "            maximizing layer (True) or a minimizing layer (False)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The score for the current search branch\n",
    "\n",
    "        tuple(int, int)\n",
    "            The best move for the current branch; (-1, -1) for no legal moves\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "            (1) You MUST use the `self.score()` method for board evaluation\n",
    "                to pass the project unit tests; you cannot call any other\n",
    "                evaluation function directly.\n",
    "        \"\"\"\n",
    "        # TERMINAL-TEST\n",
    "        if game.is_winner(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), game.get_player_location(self)\n",
    "        if game.is_loser(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), (-1,-1)\n",
    "        def max_value(game, current_depth):\n",
    "            \"\"\"Finds the best move for agent at current_depth.\n",
    "            \"\"\"\n",
    "            if self.time_left() < self.TIMER_THRESHOLD:\n",
    "                raise Timeout()\n",
    "            if game.is_winner(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if game.is_loser(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if current_depth >= depth:  \n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            score = float(\"-inf\")\n",
    "            for m in game.get_legal_moves(self):\n",
    "                score = max(score, min_value(game.forecast_move(m), current_depth+1))\n",
    "            return score\n",
    "        def min_value(game, current_depth):\n",
    "            \"\"\"Finds the best move for opponent at current_depth.\n",
    "            \"\"\"\n",
    "            if self.time_left() < self.TIMER_THRESHOLD:\n",
    "                raise Timeout()\n",
    "            if game.is_winner(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if game.is_loser(self):\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            if current_depth >= depth:\n",
    "                return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "            score = float(\"inf\")\n",
    "            for m in game.get_legal_moves():\n",
    "                score = min(score, max_value(game.forecast_move(m), current_depth+1))\n",
    "            return score\n",
    "        moves = game.get_legal_moves()\n",
    "        imm_scores = [min_value(game.forecast_move(m),1) for m in moves]\n",
    "        best_score = max(imm_scores)\n",
    "        best_move = moves[imm_scores.index(best_score)]\n",
    "        return best_score, best_move\n",
    "\n",
    "    def alphabeta(self, game, depth, alpha=float(\"-inf\"), beta=float(\"inf\"), maximizing_player=True):\n",
    "        \"\"\"Implement minimax search with alpha-beta pruning as described in the\n",
    "        lectures.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        game : isolation.Board\n",
    "            An instance of the Isolation game `Board` class representing the\n",
    "            current game state\n",
    "\n",
    "        depth : int\n",
    "            Depth is an integer representing the maximum number of plies to\n",
    "            search in the game tree before aborting\n",
    "\n",
    "        alpha : float\n",
    "            Alpha limits the lower bound of search on minimizing layers\n",
    "\n",
    "        beta : float\n",
    "            Beta limits the upper bound of search on maximizing layers\n",
    "\n",
    "        maximizing_player : bool\n",
    "            Flag indicating whether the current search depth corresponds to a\n",
    "            maximizing layer (True) or a minimizing layer (False)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The score for the current search branch\n",
    "\n",
    "        tuple(int, int)\n",
    "            The best move for the current branch; (-1, -1) for no legal moves\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "            (1) You MUST use the `self.score()` method for board evaluation\n",
    "                to pass the project unit tests; you cannot call any other\n",
    "                evaluation function directly.\n",
    "        \"\"\"\n",
    "        if self.time_left() < self.TIMER_THRESHOLD:\n",
    "            raise Timeout()\n",
    "        if game.is_winner(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), game.get_player_location(self)\n",
    "        if game.is_loser(self):\n",
    "            return self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base), (-1,-1)\n",
    "        low_score, high_score = float(\"inf\"), float('-inf')\n",
    "        best_move = (-1,-1)\n",
    "        if depth == 1:\n",
    "            if maximizing_player:\n",
    "                for move in game.get_legal_moves(self):\n",
    "                    score = self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "                    if score >= beta:\n",
    "                        return score, move\n",
    "                    if score > high_score:\n",
    "                        high_score, best_move = score, move\n",
    "                return high_score, best_move\n",
    "            else:\n",
    "                for move in game.get_legal_moves():\n",
    "                    score = self.score(game, self, self.agent_weight, self.agent_exp, self.agent_base,\n",
    "                              self.opp_weight, self.opp_exp, self.opp_base)\n",
    "                    if score <= alpha:\n",
    "                        return score, move\n",
    "                    if score < low_score:\n",
    "                        low_score, best_move = score, move\n",
    "                return low_score, best_move\n",
    "        if maximizing_player:\n",
    "            for move in game.get_legal_moves(self):\n",
    "                score, _ = self.alphabeta(game.forecast_move(move), depth-1, alpha, beta, maximizing_player=False)\n",
    "                if score>=beta:\n",
    "                    return score, move\n",
    "                if score>high_score:\n",
    "                    high_score, best_move = score, move\n",
    "                alpha = max(high_score, alpha)\n",
    "            return high_score, best_move\n",
    "        else:\n",
    "            for move in game.get_legal_moves():\n",
    "                score, _ = self.alphabeta(game.forecast_move(move), depth-1, alpha, beta, maximizing_player=True)\n",
    "                if score<=alpha:\n",
    "                    return score, move\n",
    "                if score<low_score:\n",
    "                    low_score, best_move = score, move\n",
    "                beta = min(low_score, beta)\n",
    "            return low_score, best_move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_MATCHES = 5  # number of matches against each opponent\n",
    "TIME_LIMIT = 150  # number of milliseconds before timeout\n",
    "\n",
    "TIMEOUT_WARNING = \"One or more agents lost a match this round due to \" + \\\n",
    "                  \"timeout. The get_move() function must return before \" + \\\n",
    "                  \"time_left() reaches 0 ms. You will need to leave some \" + \\\n",
    "                  \"time for the function to return, and may need to \" + \\\n",
    "                  \"increase this margin to avoid timeouts during  \" + \\\n",
    "                  \"tournament play.\"\n",
    "\n",
    "DESCRIPTION = \"\"\"\n",
    "This script evaluates the performance of the custom heuristic function by\n",
    "comparing the strength of an agent using iterative deepening (ID) search with\n",
    "alpha-beta pruning against the strength rating of agents using other heuristic\n",
    "functions.  The `ID_Improved` agent provides a baseline by measuring the\n",
    "performance of a basic agent using Iterative Deepening and the \"improved\"\n",
    "heuristic (from lecture) on your hardware.  The `Student` agent then measures\n",
    "the performance of Iterative Deepening and the custom heuristic against the\n",
    "same opponents.\n",
    "\"\"\"\n",
    "\n",
    "Agent = namedtuple(\"Agent\", [\"player\", \"name\"])\n",
    "\n",
    "\n",
    "def play_match(player1, player2):\n",
    "    \"\"\"\n",
    "    Play a \"fair\" set of matches between two agents by playing two games\n",
    "    between the players, forcing each agent to play from randomly selected\n",
    "    positions. This should control for differences in outcome resulting from\n",
    "    advantage due to starting position on the board.\n",
    "    \"\"\"\n",
    "    num_wins = {player1: 0, player2: 0}\n",
    "    num_timeouts = {player1: 0, player2: 0}\n",
    "    num_invalid_moves = {player1: 0, player2: 0}\n",
    "    games = [Board(player1, player2), Board(player2, player1)]\n",
    "\n",
    "    # initialize both games with a random move and response\n",
    "    for _ in range(2):\n",
    "        move = random.choice(games[0].get_legal_moves())\n",
    "        games[0].apply_move(move)\n",
    "        games[1].apply_move(move)\n",
    "\n",
    "    # play both games and tally the results\n",
    "    for game in games:\n",
    "        winner, _, termination = game.play(time_limit=TIME_LIMIT)\n",
    "\n",
    "        if player1 == winner:\n",
    "            num_wins[player1] += 1\n",
    "\n",
    "            if termination == \"timeout\":\n",
    "                num_timeouts[player2] += 1\n",
    "            else:\n",
    "                num_invalid_moves[player2] += 1\n",
    "\n",
    "        elif player2 == winner:\n",
    "\n",
    "            num_wins[player2] += 1\n",
    "\n",
    "            if termination == \"timeout\":\n",
    "                num_timeouts[player1] += 1\n",
    "            else:\n",
    "                num_invalid_moves[player1] += 1\n",
    "\n",
    "    if sum(num_timeouts.values()) != 0:\n",
    "        warnings.warn(TIMEOUT_WARNING)\n",
    "\n",
    "    return num_wins[player1], num_wins[player2]\n",
    "\n",
    "\n",
    "def play_round(agents, num_matches):\n",
    "    \"\"\"\n",
    "    Play one round (i.e., a single match between each pair of opponents)\n",
    "    \"\"\"\n",
    "    agent_1 = agents[-1]\n",
    "    wins = 0.\n",
    "    total = 0.\n",
    "\n",
    "    print(\"\\nPlaying Matches:\")\n",
    "    print(\"----------\")\n",
    "\n",
    "    for idx, agent_2 in enumerate(agents[:-1]):\n",
    "\n",
    "        counts = {agent_1.player: 0., agent_2.player: 0.}\n",
    "        names = [agent_1.name, agent_2.name]\n",
    "        print(\"  Match {}: {!s:^11} vs {!s:^11}\".format(idx + 1, *names), end=' ')\n",
    "\n",
    "        # Each player takes a turn going first\n",
    "        for p1, p2 in itertools.permutations((agent_1.player, agent_2.player)):\n",
    "            for _ in range(num_matches):\n",
    "                score_1, score_2 = play_match(p1, p2)\n",
    "                counts[p1] += score_1\n",
    "                counts[p2] += score_2\n",
    "                total += score_1 + score_2\n",
    "\n",
    "        wins += counts[agent_1.player]\n",
    "\n",
    "        print(\"\\tResult: {} to {}\".format(int(counts[agent_1.player]),\n",
    "                                          int(counts[agent_2.player])))\n",
    "\n",
    "    return 100. * wins / total\n",
    "\n",
    "def main(agent_weight=1, agent_exp=2, agent_base=0, opp_weight=2, opp_exp=1, opp_base=0):\n",
    "\n",
    "    HEURISTICS = [(\"Null\", null_score),\n",
    "                  (\"Open\", open_move_score),\n",
    "                  (\"Improved\", improved_score)]\n",
    "    AB_ARGS = {\"search_depth\": 5, \"method\": 'alphabeta', \"iterative\": False}\n",
    "    MM_ARGS = {\"search_depth\": 3, \"method\": 'minimax', \"iterative\": False}\n",
    "    CUSTOM_ARGS = {\"method\": 'alphabeta', 'iterative': True}\n",
    "    CUSTOM_ARGS1 = {\"method\": 'alphabeta', 'iterative': True, \n",
    "                   'agent_weight':agent_weight, 'agent_exp':agent_exp, 'agent_base':agent_base, \n",
    "                   'opp_weight':opp_weight, 'opp_exp':opp_exp, 'opp_base':opp_base}\n",
    "\n",
    "    # Create a collection of CPU agents using fixed-depth minimax or alpha beta\n",
    "    # search, or random selection.  The agent names encode the search method\n",
    "    # (MM=minimax, AB=alpha-beta) and the heuristic function (Null=null_score,\n",
    "    # Open=open_move_score, Improved=improved_score). For example, MM_Open is\n",
    "    # an agent using minimax search with the open moves heuristic.\n",
    "    mm_agents = [Agent(CustomPlayer(score_fn=h, **MM_ARGS),\n",
    "                       \"MM_\" + name) for name, h in HEURISTICS]\n",
    "    ab_agents = [Agent(CustomPlayer(score_fn=h, **AB_ARGS),\n",
    "                       \"AB_\" + name) for name, h in HEURISTICS]\n",
    "    random_agents = [Agent(RandomPlayer(), \"Random\")]\n",
    "\n",
    "    # ID_Improved agent is used for comparison to the performance of the\n",
    "    # submitted agent for calibration on the performance across different\n",
    "    # systems; i.e., the performance of the student agent is considered\n",
    "    # relative to the performance of the ID_Improved agent to account for\n",
    "    # faster or slower computers.\n",
    "    test_agents = [Agent(CustomPlayer(score_fn=improved_score, **CUSTOM_ARGS), \"ID_Improved\"),\n",
    "                   Agent(CustomPlayer1(score_fn=custom_score, **CUSTOM_ARGS), \"Student\")]\n",
    "    print(CUSTOM_ARGS1)\n",
    "\n",
    "    print(DESCRIPTION)\n",
    "    win_ratios = []\n",
    "    for agentUT in test_agents:\n",
    "        print(\"\")\n",
    "        print(\"*************************\")\n",
    "        print(\"{:^25}\".format(\"Evaluating: \" + agentUT.name))\n",
    "        print(\"*************************\")\n",
    "\n",
    "        agents = random_agents + mm_agents + ab_agents + [agentUT]\n",
    "        win_ratio = play_round(agents, NUM_MATCHES)\n",
    "        win_ratios.append(win_ratio)\n",
    "        print(\"\\n\\nResults:\")\n",
    "        print(\"----------\")\n",
    "        print(\"{!s:<15}{:>10.2f}%\".format(agentUT.name, win_ratio))\n",
    "    return win_ratios[1]/win_ratios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   agent_base |   agent_exp |   agent_weight |   opp_base |   opp_exp |   opp_weight | \n",
      "{'method': 'alphabeta', 'iterative': True, 'agent_weight': 50.608807222594976, 'agent_exp': 7.270552788673772, 'agent_base': 7.8917177342897071, 'opp_weight': 25.811625137927908, 'opp_exp': 0.62405679010844817, 'opp_base': 1.9364400936280657}\n",
      "\n",
      "This script evaluates the performance of the custom heuristic function by\n",
      "comparing the strength of an agent using iterative deepening (ID) search with\n",
      "alpha-beta pruning against the strength rating of agents using other heuristic\n",
      "functions.  The `ID_Improved` agent provides a baseline by measuring the\n",
      "performance of a basic agent using Iterative Deepening and the \"improved\"\n",
      "heuristic (from lecture) on your hardware.  The `Student` agent then measures\n",
      "the performance of Iterative Deepening and the custom heuristic against the\n",
      "same opponents.\n",
      "\n",
      "\n",
      "*************************\n",
      " Evaluating: ID_Improved \n",
      "*************************\n",
      "\n",
      "Playing Matches:\n",
      "----------\n",
      "  Match 1: ID_Improved vs   Random    \tResult: 20 to 0\n",
      "  Match 2: ID_Improved vs   MM_Null   \tResult: 15 to 5\n",
      "  Match 3: ID_Improved vs   MM_Open   \tResult: 15 to 5\n",
      "  Match 4: ID_Improved vs MM_Improved \tResult: 10 to 10\n",
      "  Match 5: ID_Improved vs   AB_Null   \tResult: 15 to 5\n",
      "  Match 6: ID_Improved vs   AB_Open   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:60: UserWarning: One or more agents lost a match this round due to timeout. The get_move() function must return before time_left() reaches 0 ms. You will need to leave some time for the function to return, and may need to increase this margin to avoid timeouts during  tournament play.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tResult: 11 to 9\n",
      "  Match 7: ID_Improved vs AB_Improved "
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_move' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36mget_move\u001b[0;34m(self, game, legal_moves, time_left)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36malphabeta\u001b[0;34m(self, game, depth, alpha, beta, maximizing_player)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximizing_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36malphabeta\u001b[0;34m(self, game, depth, alpha, beta, maximizing_player)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximizing_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36malphabeta\u001b[0;34m(self, game, depth, alpha, beta, maximizing_player)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximizing_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36malphabeta\u001b[0;34m(self, game, depth, alpha, beta, maximizing_player)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphabeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximizing_player\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36malphabeta\u001b[0;34m(self, game, depth, alpha, beta, maximizing_player)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIMER_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_winner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeout\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-41508831cd6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m         'opp_weight':(0,100), 'opp_exp':(0,10), 'opp_base':(0,10)}\n\u001b[1;32m      3\u001b[0m \u001b[0mBO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.576\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self, init_points)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0my_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-5d331c31bac2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(agent_weight, agent_exp, agent_base, opp_weight, opp_exp, opp_base)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_agents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmm_agents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mab_agents\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magentUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mwin_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_MATCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mwin_ratios\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwin_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nResults:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-5d331c31bac2>\u001b[0m in \u001b[0;36mplay_round\u001b[0;34m(agents, num_matches)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mscore_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-5d331c31bac2>\u001b[0m in \u001b[0;36mplay_match\u001b[0;34m(player1, player2)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# play both games and tally the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgame\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mwinner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_LIMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplayer1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwinner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/isolation/isolation.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, time_limit)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mmove_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_time_millis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mtime_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtime_limit\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurr_time_millis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmove_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mcurr_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_move\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegal_player_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mmove_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jasonmancuso/AIND/AIND-Isolation/game_agent.py\u001b[0m in \u001b[0;36mget_move\u001b[0;34m(self, game, legal_moves, time_left)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;31m# Handle any actions required at timeout, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbest_move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Return the best move from the last completed search iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_move' referenced before assignment"
     ]
    }
   ],
   "source": [
    "params={'agent_weight':(0,100), 'agent_exp':(0,10), 'agent_base':(0,10),\n",
    "        'opp_weight':(0,100), 'opp_exp':(0,10), 'opp_base':(0,10)}\n",
    "BO = BayesianOptimization(main, params)\n",
    "BO.maximize(init_points=5, n_iter=100, acq='ucb', kappa=2.576*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_params': {'agent_base': 7.4921842058035448,\n",
       "  'agent_exp': 7.4205991775955464,\n",
       "  'agent_weight': 82.874576048706842,\n",
       "  'opp_base': 5.7974792415172294,\n",
       "  'opp_exp': 7.6446303872568819,\n",
       "  'opp_weight': 73.420012687089994},\n",
       " 'max_val': 1.1363636363636365}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BO.res['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py:258: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 60 but corresponding boolean dimension is 61\n",
      "  self.gp.fit(self.X[ur], self.Y[ur])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 60 is out of bounds for axis 1 with size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-816bf4b0af09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ucb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkappa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.576\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jasonmancuso/anaconda/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# Find unique rows of X to avoid GP from breaking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Finding argmax of the acquisition function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 60 is out of bounds for axis 1 with size 60"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
